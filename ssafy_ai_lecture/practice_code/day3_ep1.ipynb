{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409a0e50",
   "metadata": {},
   "source": [
    "\n",
    "# 한국어 텍스트 분류: TF‑IDF와 Word2Vec 실습\n",
    "\n",
    "이 노트북에서는 간단한 한국어 문장 데이터셋을 사용하여 TF‑IDF 기반 분류기와 Word2Vec 기반 분류기를 학습해 봅니다. 데이터셋은 긍정(1)과 부정(0) 두 가지 레이블로 구성되어 있습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb306859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Hugging Face 데이터셋 주소\"\n",
    "url = 'https://huggingface.co/datasets/Blpeng/nsmc/resolve/main/ratings.csv'\n",
    "df = pd.read_csv(url)\n",
    "# NSMC 데이터는 id, document, label 세 컬럼을 포함합니다.\n",
    "df = df[['document', 'label']].rename(columns={'document': 'sentence'})\n",
    "df =df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b591704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "      <td>[어릴때보고, 지금다시봐도, 재밌어요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "      <td>[디자인을, 배우는, 학생으로, 외국디자이너와, 그들이, 일군, 전통을, 통해, 발...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "      <td>[폴리스스토리, 시리즈는, 1부터, 뉴까지, 버릴께, 하나도, 없음, 최고]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "      <td>[와, 연기가, 진짜, 개쩔구나, 지루할거라고, 생각했는데, 몰입해서, 봤다, 그래...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "      <td>[안개, 자욱한, 밤하늘에, 떠, 있는, 초승달, 같은, 영화]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>포켓 몬스터 짜가 ㅡㅡ;;</td>\n",
       "      <td>0</td>\n",
       "      <td>[포켓, 몬스터, 짜가]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>쓰.레.기</td>\n",
       "      <td>0</td>\n",
       "      <td>[쓰레기]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.</td>\n",
       "      <td>0</td>\n",
       "      <td>[완전, 사이코영화, 마지막은, 더욱더, 이, 영화의질을, 떨어트린다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "      <td>[왜난, 재미없었지, 라따뚜이, 보고나서, 스머프, 봐서, 그런가]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>포풍저그가나가신다영차영차영차</td>\n",
       "      <td>0</td>\n",
       "      <td>[포풍저그가나가신다영차영차영차]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199992 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  label  \\\n",
       "0                                     어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1   \n",
       "1       디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1   \n",
       "2                    폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1   \n",
       "3       와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1   \n",
       "4                             안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1   \n",
       "...                                                   ...    ...   \n",
       "199995                                     포켓 몬스터 짜가 ㅡㅡ;;      0   \n",
       "199996                                              쓰.레.기      0   \n",
       "199997                  완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.      0   \n",
       "199998                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0   \n",
       "199999                                    포풍저그가나가신다영차영차영차      0   \n",
       "\n",
       "                                                   tokens  \n",
       "0                                   [어릴때보고, 지금다시봐도, 재밌어요]  \n",
       "1       [디자인을, 배우는, 학생으로, 외국디자이너와, 그들이, 일군, 전통을, 통해, 발...  \n",
       "2              [폴리스스토리, 시리즈는, 1부터, 뉴까지, 버릴께, 하나도, 없음, 최고]  \n",
       "3       [와, 연기가, 진짜, 개쩔구나, 지루할거라고, 생각했는데, 몰입해서, 봤다, 그래...  \n",
       "4                     [안개, 자욱한, 밤하늘에, 떠, 있는, 초승달, 같은, 영화]  \n",
       "...                                                   ...  \n",
       "199995                                      [포켓, 몬스터, 짜가]  \n",
       "199996                                              [쓰레기]  \n",
       "199997            [완전, 사이코영화, 마지막은, 더욱더, 이, 영화의질을, 떨어트린다]  \n",
       "199998              [왜난, 재미없었지, 라따뚜이, 보고나서, 스머프, 봐서, 그런가]  \n",
       "199999                                  [포풍저그가나가신다영차영차영차]  \n",
       "\n",
       "[199992 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 간단한 전처리: 문장을 공백 기준으로 토큰화\n",
    "# 한국어 형태소 분석기가 없기 때문에 공백 기준으로 나눕니다.\n",
    "import re\n",
    "\n",
    "def tokenize(sentence):\n",
    "    # 특수 문자 제거하고 공백으로 분리\n",
    "    sentence = re.sub(r\"[^가-힣0-9\\s]\", \"\", sentence)\n",
    "    tokens = sentence.strip().split()\n",
    "    return tokens\n",
    "\n",
    "# 토큰화 적용\n",
    "\n",
    "df['tokens'] = df['sentence'].apply(tokenize)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ef1883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF‑IDF 분류 결과:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80     30076\n",
      "           1       0.81      0.75      0.78     29922\n",
      "\n",
      "    accuracy                           0.79     59998\n",
      "   macro avg       0.79      0.79      0.79     59998\n",
      "weighted avg       0.79      0.79      0.79     59998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TF‑IDF 벡터화를 이용한 분류기 학습\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 학습용과 테스트용 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sentence'], df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# TF‑IDF 벡터 생성기\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda text: text.split(), min_df=1)\n",
    "\n",
    "# 학습 데이터에 대해 벡터화 수행\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# 로지스틱 회귀 분류기 학습\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 테스트 데이터 평가\n",
    "pred = clf.predict(X_test_tfidf)\n",
    "print(\"TF‑IDF 분류 결과:\")\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427833f",
   "metadata": {},
   "source": [
    "\n",
    "## Word2Vec 모델 구현\n",
    "\n",
    "Word2Vec은 단어를 고정 길이 벡터로 표현하는 기법입니다. 대표적인 학습 방식은 **skip‑gram** 구조로, 중심 단어를 입력하여 주변 단어를 예측합니다. 여기서는 numpy를 사용하여 간단한 skip‑gram 모델을 구현합니다. 학습이 간단한 만큼 데이터셋도 작은 규모로 진행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51cd9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 수: 29984\n",
      "예시 단어 20개: ['영화', '너무', '정말', '진짜', '이', '왜', '그냥', '더', '이런', '수', '영화를', '잘', '다', '보고', '좀', '영화는', '영화가', '그', '본', '최고의']\n"
     ]
    }
   ],
   "source": [
    "# ==== Word2Vec with gensim, doc embeddings, classifier, visualization ====\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# 재현성 고정\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "sentences = df['tokens'].values.tolist()\n",
    "\n",
    "# 2. Word2Vec 학습\n",
    "# sg=0는 CBOW, sg=1은 Skip-gram\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=8,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    sg=0,\n",
    "    epochs=5  # 필요 시 늘리세요\n",
    ")\n",
    "\n",
    "# 3. vocabulary 확인\n",
    "vocab = list(w2v.wv.key_to_index.keys())\n",
    "print(f\"단어 수: {len(vocab)}\")\n",
    "print(\"예시 단어 20개:\", vocab[:20])\n",
    "\n",
    "# 4. 모델 저장과 로드 예시\n",
    "# w2v.save(\"w2v_model.model\")\n",
    "# 키드벡터만 저장하려면\n",
    "# w2v.wv.save(\"w2v_vectors.kv\")\n",
    "\n",
    "# 필요 시 다시 로드\n",
    "# w2v = Word2Vec.load(\"w2v_model.model\")\n",
    "# kv = KeyedVectors.load(\"w2v_vectors.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45ff569a-24c1-483b-aae5-d1d34456904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6863171579289482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.74      0.70     20000\n",
      "           1       0.71      0.63      0.67     19999\n",
      "\n",
      "    accuracy                           0.69     39999\n",
      "   macro avg       0.69      0.69      0.69     39999\n",
      "weighted avg       0.69      0.69      0.69     39999\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 200 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=200).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 문서 임베딩 만들기\n",
    "# 각 문서의 토큰 중 vocab에 존재하는 단어 벡터 평균\n",
    "def doc_vector(tokens, model):\n",
    "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "X = np.vstack([doc_vector(toks, w2v) for toks in sentences])\n",
    "\n",
    "# 레이블 y 준비. 이미 df['label']이 0,1 형태라면 그대로 사용\n",
    "if 'label' in df.columns:\n",
    "    y = df['label'].values\n",
    "else:\n",
    "    # 레이블이 없으면 모두 0으로 세팅. 실제 실험에서는 반드시 레이블을 제공하세요.\n",
    "    y = np.zeros(len(df), dtype=int)\n",
    "\n",
    "# 6. 분류기 학습과 평가\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y if len(np.unique(y)) > 1 else None\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    n_jobs=None  # 최신 sklearn에서는 n_jobs가 제거된 경우가 있어 옵션 생략 권장\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda328aa-fd1b-4c6b-b0f7-60d1039a3259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
