{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4ef5c-97be-440b-ac2a-e0257e671caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LM-Eval Harness 를 활용해 언어 모델 평가하기 \n",
    "# pip install vllm \n",
    "# git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
    "# cd lm-evaluation-harness\n",
    "# pip install -e .\n",
    "\n",
    "# lm_eval --model vllm \\\n",
    "#     --model_args pretrained=kakaocorp/kanana-1.5-2.1b-instruct-2505,dtype=auto \\\n",
    "#     --tasks kmmlu_direct \\\n",
    "#     --log_samples \\\n",
    "#     --output_path results \\\n",
    "#     --batch_size auto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c41cf8b-43d7-4218-a52b-6806fef6798e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KoSimpleEval 에  새로운 데이터셋 추가하기\n",
    "# short-form\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HAERAE-HUB/HRM8K\", \"GSM8K\")\n",
    "df = ds['test'].to_pandas()\n",
    "df = df[['question','answer']]\n",
    "df.columns = ['question','gold']\n",
    "df.to_csv('hrm8k_gsm.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebca3e7b-d61b-45fd-9399-5e0c3d614407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$y=\\frac{2}{x^2+x-6}$의 그래프는 몇 개의 수직 점근선을 가지나요?</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$30$의 $120\\%$와 $20$의 $130\\%$의 양의 차는 무엇인가?</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$2^8=4^x$일 때, $x$의 값은 얼마입니까?</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>등차수열 6, 10, 14, 18, ...의 100번째 항은 무엇인가요?</td>\n",
       "      <td>402.0</td>\n",
       "      <td>Level 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Madoff는 매년 일정한 이자율로 복리 계산되는 펀드에 1000달러를 투자...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Level 4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question   gold category\n",
       "0    $y=\\frac{2}{x^2+x-6}$의 그래프는 몇 개의 수직 점근선을 가지나요?     2.0  Level 3\n",
       "1         $30$의 $120\\%$와 $20$의 $130\\%$의 양의 차는 무엇인가?    10.0  Level 1\n",
       "2                      $2^8=4^x$일 때, $x$의 값은 얼마입니까?     4.0  Level 1\n",
       "3           등차수열 6, 10, 14, 18, ...의 100번째 항은 무엇인가요?  402.0  Level 2\n",
       "4  Mr. Madoff는 매년 일정한 이자율로 복리 계산되는 펀드에 1000달러를 투자...    7.0  Level 4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566c6d9c-921d-46b2-85bb-d6cceb663fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoSimpleEval 에  새로운 데이터셋 추가하기\n",
    "# mcqa\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"HAERAE-HUB/KoSimpleEval\", \"hrm_gsm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3fade7f-e535-4264-a2c1-6ad7ac9a5eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janet의 오리는 하루에 16개의 알을 낳습니다. 그녀는 매일 아침으로 3개를 먹고, 친구들을 위해 머핀을 구울 때 4개를 사용합니다. 남은 계란은 매일 농산물 시장에서 신선한 오리 알 하나당 2달러에 판매합니다. 그녀는 매일 농산물 시장에서 얼마를 버나요?\n"
     ]
    }
   ],
   "source": [
    "print(ds['test']['question'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c889bbaa-e54e-4bbc-b741-ab4b7ac658e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c1de137b534b19b6b366d56aeff3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942bd9ea283e4a898697baa57382391d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                     | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DATASET = \"hrm_gsm\"\n",
    "\n",
    "llm = LLM(\n",
    "    model='./qwen2.5-0.5b-instruct-q4_0.gguf',\n",
    "    tokenizer=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    max_model_len=2048,\n",
    "    gpu_memory_utilization=0.6,\n",
    ")\n",
    "\n",
    "# ---- load data ----------------------------------------------------------\n",
    "df = load_dataset('HAERAE-HUB/KoSimpleEval',DATASET, split='test').to_pandas().head(20)\n",
    "\n",
    "# ---- craft prompts ------------------------------------------------------\n",
    "prompts = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "\n",
    "    prompts.append([\n",
    "        {\"role\": \"system\", \"content\":\"You are a helpful Korean assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": row['question']+\"\\n최종 답을 \\\\boxed{N}의 형태로 적어주세요.\"}\n",
    "    ])\n",
    "\n",
    "# ---- generate -----------------------------------------------------------\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=2048,\n",
    "    repetition_penalty=1.01\n",
    ")\n",
    "outputs = llm.chat(prompts, sampling_params)\n",
    "df[\"response\"] = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5abc2adf-d741-4482-a101-d4fb51db8766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "## Math Evaluation\n",
    "import pandas as pd \n",
    "from math_verify import parse, verify\n",
    "\n",
    "output = []\n",
    "for _,row in df.iterrows():\n",
    "    # break\n",
    "    try:\n",
    "        response = row.response#.split('</think>')[1]\n",
    "        resp = parse(response)\n",
    "        \n",
    "        gold = parse('\\\\boxed{' + str(row.gold) + '}')\n",
    "        is_correct0 = verify(gold,resp)\n",
    "        is_correct1 = row.gold == resp[-1]\n",
    "    \n",
    "        output.append(\n",
    "            any([is_correct0,is_correct1])\n",
    "        )\n",
    "    except:\n",
    "        output.append(False)\n",
    "df['correct'] = output     \n",
    "\n",
    "overall_acc = df[\"correct\"].mean()\n",
    "print(f\"Overall accuracy: {overall_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fac1bce-426f-4fd6-8c52-38eade318fce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'math_verify'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Math Evaluation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath_verify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse, verify\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHRM_MATH--Qwen_Qwen2.5-1.5B-Instruct.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'math_verify'"
     ]
    }
   ],
   "source": [
    "## Math Evaluation\n",
    "import pandas as pd \n",
    "from math_verify import parse, verify\n",
    "\n",
    "df = pd.read_csv('HRM_MATH--Qwen_Qwen2.5-1.5B-Instruct.csv')\n",
    "output = []\n",
    "for _,row in df.iterrows():\n",
    "    # break\n",
    "    try:\n",
    "        response = row.response#.split('</think>')[1]\n",
    "        resp = parse(response)\n",
    "        \n",
    "        gold = parse('\\\\boxed{' + str(row.gold) + '}')\n",
    "        is_correct0 = verify(gold,resp)\n",
    "        is_correct1 = row.gold == resp[-1]\n",
    "    \n",
    "        output.append(\n",
    "            any([is_correct0,is_correct1])\n",
    "        )\n",
    "    except:\n",
    "        output.append(False)\n",
    "df['correct'] = output     \n",
    "\n",
    "overall_acc = df[\"correct\"].mean()\n",
    "print(f\"Overall accuracy: {overall_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827f953-0f4a-4093-b67a-6ad98dcef7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMMLU-Redux Evaluation\n",
    "import pandas as pd \n",
    "from math_verify import parse\n",
    "\n",
    "df = pd.read_csv('HRB1_0--Qwen_Qwen2.5-1.5B-Instruct.csv')\n",
    "\n",
    "output = []\n",
    "for _,row in df.iterrows():\n",
    "    try:\n",
    "        response = row.response.split('</think>')[1]\n",
    "        resp = parse(response)\n",
    "        output.append(resp[-1])\n",
    "    except:\n",
    "        output.append('-1')\n",
    "df['pred'] = output     \n",
    "\n",
    "\n",
    "# 1) Define mappings\n",
    "num2letter = {1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\", 5:'E'}\n",
    "letter2num = {v: k for k, v in num2letter.items()}\n",
    "\n",
    "# 2) Normalize both sides to numbers (1–4) in a new column pred_num\n",
    "def to_pred_num(x):\n",
    "    # if it’s already an integer or a numeric string\n",
    "    if isinstance(x, (int, float)) or (isinstance(x, str) and x.isdigit()):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    # if it’s one of the letters A–D\n",
    "    if isinstance(x, str) and x.upper() in letter2num:\n",
    "        return letter2num[x.upper()]\n",
    "    # otherwise\n",
    "    return None\n",
    "\n",
    "df[\"pred_num\"] = df[\"pred\"].apply(to_pred_num)\n",
    "\n",
    "# 3) Optionally also create gold_letter if you want letter view\n",
    "df[\"gold_letter\"] = df[\"gold\"].map(num2letter)\n",
    "\n",
    "# 4) Now mark correctness\n",
    "df[\"correct\"] = df[\"pred_num\"] == df[\"gold\"]\n",
    "\n",
    "# 5) Overall accuracy\n",
    "overall_acc = df[\"correct\"].mean()\n",
    "print(f\"Overall accuracy: {overall_acc:.2%}\")\n",
    "\n",
    "# 6) Accuracy by category\n",
    "acc_by_cat = df.groupby(\"category\")[\"correct\"].mean().sort_values(ascending=False)\n",
    "print(\"\\nAccuracy by category:\")\n",
    "print(acc_by_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
