{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732c037d-bc90-4783-9d46-8e30aaf8bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reasoning 모델 사용해보기\n",
    "#!pip install vllm \n",
    "!wget https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q2_K.gguf\n",
    "!wget https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e4594c-1104-4e7d-b408-0ddc05729f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-12 23:58:39 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-12 23:58:40 [utils.py:328] non-default args: {'tokenizer': 'Qwen/Qwen3-0.6B', 'trust_remote_code': True, 'max_model_len': 6672, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'model': './Qwen3-0.6B-Q2_K.gguf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-12 23:59:09 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 10-12 23:59:09 [config.py:278] Error retrieving safetensors: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './Qwen3-0.6B-Q2_K.gguf'., retrying 1 of 2\n",
      "ERROR 10-12 23:59:11 [config.py:276] Error retrieving safetensors: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './Qwen3-0.6B-Q2_K.gguf'.\n",
      "INFO 10-12 23:59:12 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 10-12 23:59:12 [__init__.py:1815] Using max model len 6672\n",
      "WARNING 10-12 23:59:12 [__init__.py:1217] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 10-12 23:59:29 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 10-12 23:59:31 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-12 23:59:46 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-12 23:59:46 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='./Qwen3-0.6B-Q2_K.gguf', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6672, download_dir=None, load_format=gguf, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gguf, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=./Qwen3-0.6B-Q2_K.gguf, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m WARNING 10-12 23:59:46 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-12 23:59:49 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m WARNING 10-12 23:59:49 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-12 23:59:49 [gpu_model_runner.py:2338] Starting to load model ./Qwen3-0.6B-Q2_K.gguf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1012 23:59:49.850064038 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-12 23:59:49 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:01 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:09 [gpu_model_runner.py:2392] Model loading took 0.2909 GiB and 19.035000 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:13 [backends.py:539] Using cache directory: /home/ssafy/.cache/vllm/torch_compile_cache/69a1606887/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:13 [backends.py:550] Dynamo bytecode transform time: 4.18 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:16 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.957 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:16 [monitor.py:34] torch.compile takes 4.18 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:23 [gpu_worker.py:298] Available KV cache memory: 0.71 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:24 [kv_cache_utils.py:864] GPU KV cache size: 6,672 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:24 [kv_cache_utils.py:868] Maximum concurrency for 6,672 tokens per request: 1.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████| 67/67 [00:13<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:38 [gpu_model_runner.py:3118] Graph capturing finished in 15 secs, took 0.63 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:38 [gpu_worker.py:391] Free memory on device (3.2/4.0 GiB) on startup. Desired GPU memory utilization is (0.6, 2.4 GiB). Actual usage is 0.29 GiB for weight, 1.39 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.63 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=-73431348` to fit into requested memory, or `--kv-cache-memory=786820404` to fully utilize gpu memory. Current kv cache memory in use is 765429452 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9764)\u001b[0;0m INFO 10-13 00:00:38 [core.py:218] init engine (profile, create kv cache, warmup model) took 29.82 seconds\n",
      "INFO 10-13 00:00:40 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-13 00:00:40 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    }
   ],
   "source": [
    "# pip install -U vllm transformers\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Create the LLM runner\n",
    "llm = LLM(\n",
    "    model='./Qwen3-0.6B-Q2_K.gguf',\n",
    "    tokenizer=\"Qwen/Qwen3-0.6B\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    max_model_len=6672,\n",
    "    gpu_memory_utilization=0.6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3b3dd71-256a-4283-b67a-541319ec0097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10b3220b8d24609af0cf040f7a4b178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d016b5d2a94cc7bc9926238d571152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                      | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The problem says that Wendi is making a combination of 15 cups and 25 cups each day. The question is, when she's making her 15 cups, she has 20 cups left, and the question is how much is left for her? The problem is phrased as: \"Wendi is making a combination of 15 cups and 25 cups each day. Wendi's total is 20 cups, and the question is how much is left for her?\" \n",
      "\n",
      "Wait, the original question is phrased as \"Wendi is making a combination of 15 cups and 25 cups each day. Wendi's total is 20 cups, and the question is how much is left for her?\" \n",
      "\n",
      "But the user is asking for the answer, and the initial answer was \"20 cups.\" But the user's question is phrased as: \"Wendi is making a combination of 15 cups and 25 cups each day. Wendi's total is 20 cups, and the question is how much is left for her?\" \n",
      "\n",
      "So the user is asking for the answer, but the initial answer was \"20 cups.\" \n",
      "\n",
      "But the problem is phrased as: \"Wendi is making a combination of 15 cups and 25 cups each day. Wendi's total is 20 cups, and the question is how much is left for her?\" \n",
      "\n",
      "So the answer should be 20 cups? But let's check. \n",
      "\n",
      "The problem is to find out how much is left for her, given that she made 15 cups and 25 cups each day, and her total is 20 cups. \n",
      "\n",
      "So the question is: \"Wendi is making a combination of 15 cups and 25 cups each day. Wendi's total is 20 cups, and the question is how much is left for her?\" \n",
      "\n",
      "So the answer should be 20 cups. \n",
      "\n",
      "But let's check. \n",
      "\n",
      "The original problem is: \n",
      "\n",
      "Wendi is making a combination of 15 cups and 25 cups each day. Wendi's total is 20 cups, and the question is how much is left for her?\n",
      "\n",
      "So the answer should be 20 cups. \n",
      "\n",
      "Yes, because she made 15 cups and 25 cups each day, and her total is 20 cups. \n",
      "\n",
      "So the answer should be 20 cups. \n",
      "\n",
      "But let's check again. \n",
      "\n",
      "The question is: \"Wendi is making a combination of 15 cups and 25 cups each day. Wendi's total is 20 cups, and the question is how much is left for her?\" \n",
      "\n",
      "Yes, so the answer should be 20 cups. \n",
      "\n",
      "So the answer is 20 cups. \n",
      "\n",
      "But let's check again. \n",
      "\n",
      "The problem is to find out how much is left for her, given that she made 15 cups and 25 cups each day, and her total is 20 cups. \n",
      "\n",
      "Yes, the answer is 20 cups. \n",
      "\n",
      "So the answer is 20 cups.\n",
      "</think>\n",
      "\n",
      "The problem is phrased as: \"Wendi is making a combination of 15 cups and 25 cups each day. Wendi's total is 20 cups, and the question is how much is left for her?\" \n",
      "\n",
      "The answer is **20 cups**.\n"
     ]
    }
   ],
   "source": [
    "# Set decoding params\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=4096,\n",
    "    # repetition_penalty=1.01\n",
    ")\n",
    "\n",
    "# Compose chat messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Korean assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Wendi는 하루에 닭 한 마리당 세 컵의 혼합 먹이를 줍니다. 아침에 15컵, 오후에 25컵의 먹이를 줍니다. Wendi의 닭 무리가 총 20마리일 때, 마지막 식사로 몇 컵의 먹이를 줘야 하나요?\"} # 정답은 402\n",
    "]\n",
    "# 정답은 20\n",
    "output = llm.chat(messages, sampling_params=sampling_params)\n",
    "text = output[0].outputs[0].text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a0ceb9-519d-4436-947a-7c05186b333a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38c4625295140309d5c4366658b6bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48df96c99b7b4f09b9e90f906219a1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                      | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "<think>\n",
      "<think>\n",
      "Okay, let's tackle this. The user is asking how to get to a village in the area \"Jum\" in Korea. First, I need to figure out what \"Jum\" refers to. It could be a specific area or a place that's popular in Korean culture. Since the user is asking about getting to a village in Korea, the answer should focus on the Korean village in Jum. \n",
      "\n",
      "The user might be looking for information on how to get to a village in Korea, perhaps in the context of a Korean village or a similar area. The answer should be structured, maybe in Korean, but since I'm writing in English, it should be clear and helpful.\n",
      "\n",
      "Since the user is asking about the \"Jum\" area, the answer should mention the area's location, maybe its geography, and the challenges or ways to get there. Also, since it's a village, maybe including some specific details about the village in Jum would be helpful. \n",
      "\n",
      "It's important to mention that the answer is in Korean, but since the user is asking in English, the answer should be in English as well. Also, the answer should include some specific information about the village in Jum.\n",
      "\n",
      "Since the user is asking in Korean, but the answer is in English, it's better to present it in English. However, since the original answer was in Korean, it's better to present it in Korean. \n",
      "\n",
      "The answer should include the location of the village in Jum and the steps to get there. The answer should be clear and easy to understand, without any jargon. \n",
      "\n",
      "Also, since the user is asking in Korean, but the answer is in English, it's better to present it in English. \n",
      "\n",
      "The answer should include the location, the steps to get there, and any specific details about the village in Jum. \n",
      "\n",
      "So, the answer is structured, and the answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer should include the specific details about the village in Jum and the steps to get there. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "The answer is in Korean, but since it's written in English, it's better. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compose chat messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Korean assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"잠실에서 신촌으로 가려면 어떻게 해야해?\"}\n",
    "]\n",
    "\n",
    "output = llm.chat(messages, sampling_params=sampling_params)\n",
    "text = output[0].outputs[0].text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79385509-7a03-4634-b865-5d3b68ecac7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6280fc6a-673a-4f3e-be74-ee49d285656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import os\n",
    "from litellm import completion\n",
    "\n",
    "os.environ['OPENROUTER_API_KEY'] = \"\"\n",
    "\n",
    "\n",
    "responses = completion(\n",
    "    model=\"openrouter/qwen/qwen3-235b-a22b:free\",\n",
    "    messages=[{ \"content\": \"1+1이 2임을 증명하시요.\",\"role\": \"user\"}]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44dec0d4-f594-4822-a152-f8a63cfe88b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 오류가 나는 경우 재 실행 필요\n",
    "think, response = responses.choices[0].message.content.split('</think>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27b6db85-2799-4a1d-b54d-6bb7cc5bc4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>Okay, I need to prove that 1+1 equals 2. Let me think about how to approach this.\n",
      "\n",
      "First, I remember that in mathematics, basic arithmetic operations like addition are defined based on axioms. The most common foundation for this is the Peano axioms, which define the natural numbers. Let me recall what those axioms are.\n",
      "\n",
      "The Peano axioms start with 0 as a natural number, and then there's a successor function S(n) that gives the next number. So, 1 is S(0), 2 is S(1) which is S(S(0)), and so on.\n",
      "\n",
      "Addition is defined recursively. The definition usually goes like this: for any natural number a, a + 0 = a, and a + S(b) = S(a + b).\n",
      "\n",
      "So, applying this to 1 + 1. Since 1 is S(0), then 1 + 1 = 1 + S(0). Using the recursive definition, this should be S(1 + 0). And since a + 0 = a, 1 + 0 = 1, so S(1 + 0) = S(1) = 2.\n",
      "\n",
      "Wait, let me check that again. Let's define 1 as S(0). Then, 1 + 1 = S(0) + S(0). According to the addition definition, a + S(b) = S(a + b). So here, a is S(0) and b is 0. So S(0) + S(0) = S(S(0) + 0). Then S(0) + 0 = S(0), so it becomes S(S(0)) which is 2. Yes, that makes sense.\n",
      "\n",
      "Alternatively, in the context of set theory, natural numbers can be defined as sets. For example, 0 is the empty set, 1 is {0}, 2 is {0, 1}, etc. Addition can be defined as the cardinality of the union of two disjoint sets. So if you have two sets each with one element, their union has two elements, hence 1 + 1 = 2.\n",
      "\n",
      "But maybe the user is expecting a more basic proof, like from first principles. In Russell and Whitehead's Principia Mathematica, they take a long time to prove 1+1=2, but that's in the context of set theory and logic. However, for most practical purposes, the Peano axioms approach is sufficient.\n",
      "\n",
      "Wait, the user is asking for a proof, so I need to lay out the steps clearly. Let me structure it step by step using Peano axioms.\n",
      "\n",
      "1. Define the natural numbers with 0 as the first element.\n",
      "2. Define the successor function S(n).\n",
      "3. Define 1 as S(0), 2 as S(1) = S(S(0)).\n",
      "4. Define addition: a + 0 = a, and a + S(b) = S(a + b).\n",
      "5. Then, 1 + 1 = S(0) + S(0) = S(S(0) + 0) [by the second addition rule, since S(0) is a and 0 is b]\n",
      "6. S(0) + 0 = S(0) [by the first addition rule]\n",
      "7. Therefore, S(S(0) + 0) = S(S(0)) = 2\n",
      "8. Hence, 1 + 1 = 2.\n",
      "\n",
      "That seems solid. I should present this in a clear, step-by-step manner. Maybe also mention that in different mathematical frameworks, the proof might vary, but Peano axioms are the standard for this kind of basic arithmetic.\n",
      "\n",
      "Wait, the user is writing in Korean, so I should respond in Korean. Let me make sure to translate the steps properly.\n",
      "\n",
      "Also, the user might not be familiar with Peano axioms, so I should explain them briefly. But since it's a proof, it's necessary to reference the axioms.\n",
      "\n",
      "Let me check if there's another way, maybe using group theory or something else, but Peano is the most straightforward.\n",
      "\n",
      "Alternatively, using the definition of integers. But 1 and 2 are natural numbers, so Peano is appropriate.\n",
      "\n",
      "I think the Peano axiom approach is the way to go here. Let me structure the answer in Korean, step by step, explaining each part.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e6a4e5a-9391-48a4-b512-927ab39099bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "다음은 1 + 1 = 2임을 **페아노 공리계**(Peano axioms) 기반으로 수학적으로 증명한 과정입니다. 이는 자연수의 기초를 정의하는 표준적 방법입니다.\n",
      "\n",
      "---\n",
      "\n",
      "### **정의 및 준비**\n",
      "1. **0을 자연수의 시작으로 정의**합니다.\n",
      "2. **후임자 함수 $ S(n) $**를 도입합니다.  \n",
      "   - $ S(n) $는 $ n $의 \"다음 수\"를 의미합니다.  \n",
      "   - 예: $ S(0) = 1 $, $ S(1) = 2 $, $ S(2) = 3 $, ...\n",
      "3. **덧셈 연산을 재귀적으로 정의**합니다.  \n",
      "   - (a) 임의의 자연수 $ a $에 대해 $ a + 0 = a $.  \n",
      "   - (b) 임의의 자연수 $ a, b $에 대해 $ a + S(b) = S(a + b) $.\n",
      "\n",
      "---\n",
      "\n",
      "### **증명 단계**\n",
      "1. $ 1 $과 $ 2 $를 페아노 공리계로 표현:  \n",
      "   - $ 1 \\equiv S(0) $,  \n",
      "   - $ 2 \\equiv S(S(0)) $.\n",
      "\n",
      "2. **$ 1 + 1 $ 계산**:  \n",
      "   $$\n",
      "   1 + 1 = S(0) + S(0)\n",
      "   $$  \n",
      "   덧셈의 재귀적 정의 (b)에 따라:  \n",
      "   $$\n",
      "   S(0) + S(0) = S\\big(S(0) + 0\\big)\n",
      "   $$  \n",
      "\n",
      "3. **$ S(0) + 0 $ 계산**:  \n",
      "   덧셈의 정의 (a)에 따라:  \n",
      "   $$\n",
      "   S(0) + 0 = S(0) = 1\n",
      "   $$  \n",
      "\n",
      "4. **결과 대입**:  \n",
      "   $$\n",
      "   S\\big(S(0) + 0\\big) = S\\big(S(0)\\big) = S(1) = 2\n",
      "   $$  \n",
      "\n",
      "5. **최종 결론**:  \n",
      "   $$\n",
      "   1 + 1 = 2\n",
      "   $$\n",
      "\n",
      "---\n",
      "\n",
      "### **핵심 논리**\n",
      "- 이 증명은 **자연수의 공리적 정의**와 **덧셈의 재귀적 규칙**에 완전히 의존합니다.  \n",
      "- 페아노 공리계는 \"1 + 1 = 2\"가 인간의 직관이 아닌 **수학적 엄밀성**에서 비롯됨을 보여줍니다.  \n",
      "- 러셀과 화이트헤드의 《수학 원리》(Principia Mathematica)에서는 이 명제를 370페이지에 걸쳐 증명했으나, 페아노 공리계는 훨씬 간결한 토대를 제공합니다.\n",
      "\n",
      "이로써 $ 1 + 1 = 2 $는 자연수 체계 내에서 **증명 가능하며 필연적인 진술**임이 확인되었습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "959a6400-40c0-457e-b9ce-1dd34bf3454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import os\n",
    "from litellm import batch_completion_models\n",
    "\n",
    "response = batch_completion_models(\n",
    "    models=[\n",
    "        \"openrouter/deepseek/deepseek-r1-0528-qwen3-8b:free\",\n",
    "        \"openrouter/openai/gpt-oss-20b:free\",\n",
    "        \"openrouter/nvidia/nemotron-nano-9b-v2:free\",\n",
    "        \"openrouter/qwen/qwen3-30b-a3b:free\",\n",
    "    ], \n",
    "    messages=[{\"role\": \"user\", \"content\": \"prove 1+1\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23155d30-2aa0-4242-86f9-fe3ac0957afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import os\n",
    "from litellm import batch_completion\n",
    "\n",
    "responses = batch_completion(\n",
    "    model=\"openrouter/qwen/qwen3-30b-a3b:free\",\n",
    "    messages = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"good morning? \"\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"what's the time? \"\n",
    "            }\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d7fb6-3bba-4b27-bb45-5dfe403a630f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
